{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis and Part of Speech Tagging\n",
    "\n",
    "In this notebook, we will create some features for our classification model by extracting information about the sentiment and syntactical makeup of these titles and texts.\n",
    "\n",
    "### I. [Preprocessing](#Preprocessing)\n",
    "### II. [Sentiment analysis](#Sentiment-analysis)\n",
    "### III. [Part of speech tagging](#Part-of-speech-tagging)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading in libraries and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import spacy\n",
    "\n",
    "from nltk.tag import pos_tag\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Git would not allow us to push a CSV containing the entire dataset so we had to split it up by X and y as well as by train and test sets. Below, we read in all of those files and put them back into one dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.read_csv('../datasets/X_train.csv')\n",
    "X_test = pd.read_csv('../datasets/X_test.csv')\n",
    "y_train = pd.read_csv('../datasets/y_train.csv')\n",
    "y_test = pd.read_csv('../datasets/y_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train['train_dataset'] = 1\n",
    "y_test['train_dataset'] = 0\n",
    "\n",
    "X = pd.concat([X_train, X_test])\n",
    "y = pd.concat([y_train, y_test])\n",
    "\n",
    "X.reset_index(drop=True, inplace=True)\n",
    "y.reset_index(drop=True, inplace=True)\n",
    "\n",
    "df = pd.concat([X, y], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>date</th>\n",
       "      <th>domestic</th>\n",
       "      <th>title_word_count</th>\n",
       "      <th>text_word_count</th>\n",
       "      <th>title_uppercase_count</th>\n",
       "      <th>title_lowercase_count</th>\n",
       "      <th>title_all_letter_count</th>\n",
       "      <th>title_special_count</th>\n",
       "      <th>...</th>\n",
       "      <th>text_all_letter_count</th>\n",
       "      <th>text_special_count</th>\n",
       "      <th>text_!</th>\n",
       "      <th>text_?</th>\n",
       "      <th>text_#</th>\n",
       "      <th>text_%</th>\n",
       "      <th>text_$</th>\n",
       "      <th>text_parentheses</th>\n",
       "      <th>is_true</th>\n",
       "      <th>train_dataset</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Trump Just Accidentally Gave America The Grea...</td>\n",
       "      <td>Donald Trump wanted to give his supporters a s...</td>\n",
       "      <td>2016-04-24</td>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "      <td>485</td>\n",
       "      <td>13</td>\n",
       "      <td>51</td>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>2021</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>U.N. torture envoy concerned at water-boarding...</td>\n",
       "      <td>GENEVA (Reuters) - The U.N. torture investigat...</td>\n",
       "      <td>2016-03-09</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>377</td>\n",
       "      <td>4</td>\n",
       "      <td>50</td>\n",
       "      <td>54</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1895</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Robert Kennedy Jr. says tapped by Trump to hea...</td>\n",
       "      <td>CHICAGO (Reuters) - Vaccination skeptic Robert...</td>\n",
       "      <td>2017-01-10</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>690</td>\n",
       "      <td>4</td>\n",
       "      <td>53</td>\n",
       "      <td>57</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>3501</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>More arrests in apparent Saudi campaign agains...</td>\n",
       "      <td>(Reuters) - Saudi Arabia has detained more cle...</td>\n",
       "      <td>2017-09-12</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>539</td>\n",
       "      <td>2</td>\n",
       "      <td>55</td>\n",
       "      <td>57</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>2702</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Illinois ends spring session without a FY 2017...</td>\n",
       "      <td>SPRINGFIELD, Ill. (Reuters) - The Democrat-con...</td>\n",
       "      <td>2016-05-31</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>592</td>\n",
       "      <td>3</td>\n",
       "      <td>38</td>\n",
       "      <td>41</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>3020</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0   Trump Just Accidentally Gave America The Grea...   \n",
       "1  U.N. torture envoy concerned at water-boarding...   \n",
       "2  Robert Kennedy Jr. says tapped by Trump to hea...   \n",
       "3  More arrests in apparent Saudi campaign agains...   \n",
       "4  Illinois ends spring session without a FY 2017...   \n",
       "\n",
       "                                                text        date  domestic  \\\n",
       "0  Donald Trump wanted to give his supporters a s...  2016-04-24         1   \n",
       "1  GENEVA (Reuters) - The U.N. torture investigat...  2016-03-09         1   \n",
       "2  CHICAGO (Reuters) - Vaccination skeptic Robert...  2017-01-10         1   \n",
       "3  (Reuters) - Saudi Arabia has detained more cle...  2017-09-12         0   \n",
       "4  SPRINGFIELD, Ill. (Reuters) - The Democrat-con...  2016-05-31         1   \n",
       "\n",
       "   title_word_count  text_word_count  title_uppercase_count  \\\n",
       "0                14              485                     13   \n",
       "1                10              377                      4   \n",
       "2                12              690                      4   \n",
       "3                 9              539                      2   \n",
       "4                 9              592                      3   \n",
       "\n",
       "   title_lowercase_count  title_all_letter_count  title_special_count  ...  \\\n",
       "0                     51                      64                    0  ...   \n",
       "1                     50                      54                    0  ...   \n",
       "2                     53                      57                    0  ...   \n",
       "3                     55                      57                    0  ...   \n",
       "4                     38                      41                    0  ...   \n",
       "\n",
       "   text_all_letter_count  text_special_count  text_!  text_?  text_#  text_%  \\\n",
       "0                   2021                   5       0       1       0       0   \n",
       "1                   1895                   2       0       0       0       0   \n",
       "2                   3501                   7       1       0       0       0   \n",
       "3                   2702                   4       0       0       0       0   \n",
       "4                   3020                  10       0       0       0       0   \n",
       "\n",
       "   text_$  text_parentheses  is_true  train_dataset  \n",
       "0       0                 4        0              1  \n",
       "1       0                 2        1              1  \n",
       "2       0                 6        1              1  \n",
       "3       0                 4        1              1  \n",
       "4       6                 4        1              1  \n",
       "\n",
       "[5 rows x 28 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>domestic</th>\n",
       "      <th>title_word_count</th>\n",
       "      <th>text_word_count</th>\n",
       "      <th>title_uppercase_count</th>\n",
       "      <th>title_lowercase_count</th>\n",
       "      <th>title_all_letter_count</th>\n",
       "      <th>title_special_count</th>\n",
       "      <th>title_!</th>\n",
       "      <th>title_?</th>\n",
       "      <th>title_#</th>\n",
       "      <th>...</th>\n",
       "      <th>text_all_letter_count</th>\n",
       "      <th>text_special_count</th>\n",
       "      <th>text_!</th>\n",
       "      <th>text_?</th>\n",
       "      <th>text_#</th>\n",
       "      <th>text_%</th>\n",
       "      <th>text_$</th>\n",
       "      <th>text_parentheses</th>\n",
       "      <th>is_true</th>\n",
       "      <th>train_dataset</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>39858.000000</td>\n",
       "      <td>39858.000000</td>\n",
       "      <td>39858.000000</td>\n",
       "      <td>39858.000000</td>\n",
       "      <td>39858.000000</td>\n",
       "      <td>39858.000000</td>\n",
       "      <td>39858.000000</td>\n",
       "      <td>39858.000000</td>\n",
       "      <td>39858.000000</td>\n",
       "      <td>39858.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>39858.000000</td>\n",
       "      <td>39858.000000</td>\n",
       "      <td>39858.000000</td>\n",
       "      <td>39858.000000</td>\n",
       "      <td>39858.000000</td>\n",
       "      <td>39858.000000</td>\n",
       "      <td>39858.000000</td>\n",
       "      <td>39858.000000</td>\n",
       "      <td>39858.000000</td>\n",
       "      <td>39858.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.729816</td>\n",
       "      <td>12.164685</td>\n",
       "      <td>416.125847</td>\n",
       "      <td>13.750489</td>\n",
       "      <td>49.146796</td>\n",
       "      <td>62.897285</td>\n",
       "      <td>0.333785</td>\n",
       "      <td>0.059963</td>\n",
       "      <td>0.033318</td>\n",
       "      <td>0.015355</td>\n",
       "      <td>...</td>\n",
       "      <td>1983.638717</td>\n",
       "      <td>5.191204</td>\n",
       "      <td>0.366451</td>\n",
       "      <td>0.622209</td>\n",
       "      <td>0.173265</td>\n",
       "      <td>0.058483</td>\n",
       "      <td>0.428346</td>\n",
       "      <td>3.542451</td>\n",
       "      <td>0.532164</td>\n",
       "      <td>0.749987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.444060</td>\n",
       "      <td>3.764510</td>\n",
       "      <td>358.401114</td>\n",
       "      <td>14.581771</td>\n",
       "      <td>13.173449</td>\n",
       "      <td>17.801389</td>\n",
       "      <td>0.704135</td>\n",
       "      <td>0.262801</td>\n",
       "      <td>0.189532</td>\n",
       "      <td>0.128351</td>\n",
       "      <td>...</td>\n",
       "      <td>1729.653051</td>\n",
       "      <td>12.849245</td>\n",
       "      <td>1.409910</td>\n",
       "      <td>1.753961</td>\n",
       "      <td>1.041448</td>\n",
       "      <td>0.810225</td>\n",
       "      <td>1.769956</td>\n",
       "      <td>10.553744</td>\n",
       "      <td>0.498971</td>\n",
       "      <td>0.433025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>213.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>43.000000</td>\n",
       "      <td>52.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1009.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>374.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>59.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1771.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>524.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>56.000000</td>\n",
       "      <td>71.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>2482.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>45.000000</td>\n",
       "      <td>8436.000000</td>\n",
       "      <td>137.000000</td>\n",
       "      <td>167.000000</td>\n",
       "      <td>233.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>42044.000000</td>\n",
       "      <td>1756.000000</td>\n",
       "      <td>133.000000</td>\n",
       "      <td>94.000000</td>\n",
       "      <td>53.000000</td>\n",
       "      <td>122.000000</td>\n",
       "      <td>129.000000</td>\n",
       "      <td>1526.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           domestic  title_word_count  text_word_count  title_uppercase_count  \\\n",
       "count  39858.000000      39858.000000     39858.000000           39858.000000   \n",
       "mean       0.729816         12.164685       416.125847              13.750489   \n",
       "std        0.444060          3.764510       358.401114              14.581771   \n",
       "min        0.000000          3.000000         1.000000               1.000000   \n",
       "25%        0.000000         10.000000       213.000000               3.000000   \n",
       "50%        1.000000         11.000000       374.000000               6.000000   \n",
       "75%        1.000000         14.000000       524.000000              20.000000   \n",
       "max        1.000000         45.000000      8436.000000             137.000000   \n",
       "\n",
       "       title_lowercase_count  title_all_letter_count  title_special_count  \\\n",
       "count           39858.000000            39858.000000         39858.000000   \n",
       "mean               49.146796               62.897285             0.333785   \n",
       "std                13.173449               17.801389             0.704135   \n",
       "min                 0.000000               12.000000             0.000000   \n",
       "25%                43.000000               52.000000             0.000000   \n",
       "50%                50.000000               59.000000             0.000000   \n",
       "75%                56.000000               71.000000             0.000000   \n",
       "max               167.000000              233.000000             6.000000   \n",
       "\n",
       "            title_!       title_?       title_#  ...  text_all_letter_count  \\\n",
       "count  39858.000000  39858.000000  39858.000000  ...           39858.000000   \n",
       "mean       0.059963      0.033318      0.015355  ...            1983.638717   \n",
       "std        0.262801      0.189532      0.128351  ...            1729.653051   \n",
       "min        0.000000      0.000000      0.000000  ...               0.000000   \n",
       "25%        0.000000      0.000000      0.000000  ...            1009.000000   \n",
       "50%        0.000000      0.000000      0.000000  ...            1771.000000   \n",
       "75%        0.000000      0.000000      0.000000  ...            2482.000000   \n",
       "max        4.000000      3.000000      4.000000  ...           42044.000000   \n",
       "\n",
       "       text_special_count        text_!        text_?        text_#  \\\n",
       "count        39858.000000  39858.000000  39858.000000  39858.000000   \n",
       "mean             5.191204      0.366451      0.622209      0.173265   \n",
       "std             12.849245      1.409910      1.753961      1.041448   \n",
       "min              0.000000      0.000000      0.000000      0.000000   \n",
       "25%              2.000000      0.000000      0.000000      0.000000   \n",
       "50%              3.000000      0.000000      0.000000      0.000000   \n",
       "75%              6.000000      0.000000      1.000000      0.000000   \n",
       "max           1756.000000    133.000000     94.000000     53.000000   \n",
       "\n",
       "             text_%        text_$  text_parentheses       is_true  \\\n",
       "count  39858.000000  39858.000000      39858.000000  39858.000000   \n",
       "mean       0.058483      0.428346          3.542451      0.532164   \n",
       "std        0.810225      1.769956         10.553744      0.498971   \n",
       "min        0.000000      0.000000          0.000000      0.000000   \n",
       "25%        0.000000      0.000000          2.000000      0.000000   \n",
       "50%        0.000000      0.000000          2.000000      1.000000   \n",
       "75%        0.000000      0.000000          4.000000      1.000000   \n",
       "max      122.000000    129.000000       1526.000000      1.000000   \n",
       "\n",
       "       train_dataset  \n",
       "count   39858.000000  \n",
       "mean        0.749987  \n",
       "std         0.433025  \n",
       "min         0.000000  \n",
       "25%         0.250000  \n",
       "50%         1.000000  \n",
       "75%         1.000000  \n",
       "max         1.000000  \n",
       "\n",
       "[8 rows x 25 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment analysis\n",
    "\n",
    "Now that we have the dataframe read in, we will create columns with sentiment scores for both the titles and text of the articles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title Sentiment Analysis\n",
      "0\n",
      "5000\n",
      "10000\n",
      "15000\n",
      "20000\n",
      "25000\n",
      "30000\n",
      "35000\n"
     ]
    }
   ],
   "source": [
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "df['title_sa_neg'] = 0\n",
    "df['title_sa_pos'] = 0\n",
    "df['title_sa_neu'] = 0\n",
    "df['title_sa_compound'] = 0\n",
    "\n",
    "print('Title Sentiment Analysis')\n",
    "for i, t in enumerate(df.title.values):\n",
    "    vs = analyzer.polarity_scores(t)\n",
    "    df.loc[i, 'title_sa_neg'] = vs['neg']\n",
    "    df.loc[i, 'title_sa_pos'] = vs['pos']\n",
    "    df.loc[i, 'title_sa_neu'] = vs['neu']\n",
    "    df.loc[i, 'title_sa_compound'] = vs['compound']\n",
    "    if (i % 5000) == 0:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text Sentiment Analysis\n",
      "0\n",
      "5000\n",
      "10000\n",
      "15000\n",
      "20000\n",
      "25000\n",
      "30000\n",
      "35000\n"
     ]
    }
   ],
   "source": [
    "df['text_sa_neg'] = 0\n",
    "df['text_sa_pos'] = 0\n",
    "df['text_sa_neu'] = 0\n",
    "df['text_sa_compound'] = 0\n",
    "\n",
    "print('Text Sentiment Analysis')        \n",
    "for i, t in enumerate(df.text.values):\n",
    "    vs = analyzer.polarity_scores(t)\n",
    "    df.loc[i, 'text_sa_neg'] = vs['neg']\n",
    "    df.loc[i, 'text_sa_pos'] = vs['pos']\n",
    "    df.loc[i, 'text_sa_neu'] = vs['neu']\n",
    "    df.loc[i, 'text_sa_neu'] = vs['compound']\n",
    "    if (i % 5000) == 0:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part of speech tagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to have more features and potentially improve the accuracy of our model, we can apply the part of speech tagging extraction feature. It is also a way to analyze the grammatical structure of each sentences and compare the syntax between real news articles and fake news articles. Each articles will be assign a ratio for each tag like verbs, nouns and adverbs: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "5000\n",
      "10000\n",
      "15000\n",
      "20000\n",
      "25000\n",
      "30000\n",
      "35000\n"
     ]
    }
   ],
   "source": [
    "# parsing parts of speech with spacy\n",
    "en_nlp = spacy.load('en')\n",
    "\n",
    "parsed_quotes = []\n",
    "for i, parsed in enumerate(en_nlp.pipe(df.text.values, batch_size=50, n_threads=4)):\n",
    "    assert parsed.is_parsed\n",
    "    if (i % 5000) == 0:\n",
    "        print(i)\n",
    "    parsed_quotes.append(parsed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating proportion column for each part of speech\n",
    "\n",
    "unique_pos = []\n",
    "\n",
    "for parsed in parsed_quotes:\n",
    "    unique_pos.extend([t.pos_ for t in parsed])\n",
    "    \n",
    "unique_pos = np.unique(unique_pos)\n",
    "\n",
    "for pos in unique_pos:\n",
    "    df[pos+'_prop'] = 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "5000\n",
      "10000\n",
      "15000\n",
      "20000\n",
      "25000\n",
      "30000\n",
      "35000\n"
     ]
    }
   ],
   "source": [
    "# looping through rows and adding proportions to each POS columns\n",
    "\n",
    "for i, parsed in enumerate(parsed_quotes):\n",
    "    if (i % 5000) == 0:\n",
    "        print(i)\n",
    "    parsed_len = len(parsed)\n",
    "    for pos in unique_pos:\n",
    "        count = len([x for x in parsed if x.pos_ == pos])\n",
    "        try:\n",
    "            df.loc[i, pos+'_prop'] = float(count)/parsed_len    \n",
    "        except:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now save the results in the form of new columns and export the data frame to a csv file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train = df[df.train_dataset == 1].drop(columns = ['is_true', 'train_dataset'])\n",
    "# X_test = df[df.train_dataset == 0].drop(columns = ['is_true', 'train_dataset'])\n",
    "\n",
    "# X_train.to_csv('X_train_w_SA.csv', index = False)\n",
    "# X_test.to_csv('X_test_w_SA.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll move onto vectorizing the text and narrowing those vectorized columns into the most important features to bring into our final classification model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
